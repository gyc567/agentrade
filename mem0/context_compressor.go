package mem0

import (
	"fmt"
	"log"
	"sort"
	"strings"
	"sync"
	"time"
)

// ContextCompressor P0ä¿®å¤#2: Tokené¢„ç®—ç®¡ç†
// ä½œç”¨: æŠŠMem0è¿”å›çš„3400 tokenså‹ç¼©åˆ°700 tokens
// é˜²æ­¢: Tokené¢„ç®—è¶…é™,AIæ¨¡å‹ä¸Šä¸‹æ–‡çˆ†ç‚¸
// ç­–ç•¥: æŒ‰ç›¸å…³æ€§æ’åº â†’ ç´¯ç§¯token â†’ å»é‡ â†’ å»å†—ä½™
type ContextCompressor struct {
	maxTokens     int
	tokenPerChar  float64 // å¹³å‡æ¯ä¸ªå­—ç¬¦çš„tokenæ•°
	maxMemories   int     // æœ€å¤šä¿ç•™çš„è®°å¿†æ•°
	deduplicator  *Deduplicator
	metrics       *CompressionMetrics
	mu            sync.RWMutex
}

// CompressionMetrics å‹ç¼©æŒ‡æ ‡
type CompressionMetrics struct {
	CompressionRuns  int64
	AvgInputTokens   float64
	AvgOutputTokens  float64
	AvgCompressionRatio float64
	TotalRemoved     int64
	LastCompressionAt *time.Time
	mu               sync.RWMutex
}

// Deduplicator å»é‡å™¨ (LRUç¼“å­˜)
type Deduplicator struct {
	seenContent map[string]bool // å·²è§è¿‡çš„å†…å®¹Hash
	addedOrder  []string        // æ·»åŠ é¡ºåº (ç”¨äºLRUæ·˜æ±°)
	similarity  float64          // ç›¸ä¼¼åº¦é˜ˆå€¼(0-1)
	maxSize     int              // âœ… ä¿®å¤: æœ€å¤§å®¹é‡é™åˆ¶ (é˜²æ­¢å†…å­˜æ³„æ¼)
	mu          sync.RWMutex
}

// CompressionResult å‹ç¼©ç»“æœ
type CompressionResult struct {
	Memories       []Memory       // ä¿ç•™çš„è®°å¿†
	InputTokens    int            // è¾“å…¥tokenæ•°
	OutputTokens   int            // è¾“å‡ºtokenæ•°
	CompressRatio  float64        // å‹ç¼©ç‡ (output/input)
	RemovedCount   int            // ç§»é™¤çš„è®°å¿†æ•°
	DeduplicatedCount int         // å»é‡çš„è®°å¿†æ•°
	Timestamp      time.Time
}

// NewContextCompressor åˆ›å»ºä¸Šä¸‹æ–‡å‹ç¼©å™¨
func NewContextCompressor(maxTokens int) *ContextCompressor {
	return &ContextCompressor{
		maxTokens:    maxTokens,
		tokenPerChar: 0.25,  // å¹³å‡4ä¸ªå­—ç¬¦ = 1ä¸ªtoken
		maxMemories:  20,    // æœ€å¤šä¿ç•™20æ¡è®°å¿†
		deduplicator: &Deduplicator{
			seenContent: make(map[string]bool),
			addedOrder:  make([]string, 0),
			similarity:  0.85,  // 85%ç›¸ä¼¼è§†ä¸ºé‡å¤
			maxSize:     5000,  // âœ… ä¿®å¤: LRUæœ€å¤§å®¹é‡ (é˜²æ­¢æ— é™å¢é•¿)
		},
		metrics: &CompressionMetrics{},
	}
}

// Compress å‹ç¼©Mem0è¿”å›çš„ç»“æœ
func (cc *ContextCompressor) Compress(memories []Memory) CompressionResult {
	startTime := time.Now()

	result := CompressionResult{
		Memories:  make([]Memory, 0),
		Timestamp: startTime,
	}

	if len(memories) == 0 {
		cc.recordMetrics(result)
		return result
	}

	// Step 1: è®¡ç®—è¾“å…¥tokenæ•°
	for _, m := range memories {
		result.InputTokens += cc.estimateTokens(m.Content)
		if m.Metadata != nil {
			for _, v := range m.Metadata {
				result.InputTokens += cc.estimateTokens(fmt.Sprintf("%v", v))
			}
		}
	}

	log.Printf("ğŸ“¥ è¾“å…¥: %dæ¡è®°å¿†, %d tokens", len(memories), result.InputTokens)

	// Step 2: æŒ‰ç›¸å…³æ€§æ’åº
	sorted := cc.sortByRelevance(memories)

	// Step 3: å»é‡ + ç´¯ç§¯tokenç›´åˆ°è¾¾åˆ°limit
	currentTokens := 0
	deduped := 0
	kept := 0

	for _, m := range sorted {
		// æ£€æŸ¥å»é‡
		if cc.deduplicator.IsDuplicate(m.Content) {
			deduped++
			result.DeduplicatedCount++
			log.Printf("  âš ï¸ å»é‡: %s (ç›¸ä¼¼åº¦é«˜)", idPrefix(m.ID))
			continue
		}

		memTokens := cc.estimateTokens(m.Content)

		// æ£€æŸ¥æ˜¯å¦è¶…è¿‡tokené™åˆ¶
		if currentTokens+memTokens > cc.maxTokens || kept >= cc.maxMemories {
			result.RemovedCount++
			log.Printf("  âŒ ç§»é™¤: %s (è¶…tokené™åˆ¶æˆ–æ•°é‡é™åˆ¶)", idPrefix(m.ID))
			continue
		}

		// ä¿ç•™æ­¤è®°å¿†
		result.Memories = append(result.Memories, m)
		currentTokens += memTokens
		kept++
		result.OutputTokens += memTokens

		cc.deduplicator.Add(m.Content)
		log.Printf("  âœ… ä¿ç•™: %s (Q=%.2f, %d tokens)", idPrefix(m.ID), m.QualityScore, memTokens)
	}

	// Step 4: è®¡ç®—å‹ç¼©ç‡
	if result.InputTokens > 0 {
		result.CompressRatio = float64(result.OutputTokens) / float64(result.InputTokens)
	}

	duration := time.Since(startTime)
	log.Printf("âœ… å‹ç¼©å®Œæˆ: %d/%dä¿ç•™, %då»é‡, %dç§»é™¤ (è€—æ—¶: %.0fms)",
		kept, len(memories), deduped, result.RemovedCount, duration.Seconds()*1000)
	log.Printf("   Input: %d tokens â†’ Output: %d tokens (æ¯”ç‡: %.1f%%)",
		result.InputTokens, result.OutputTokens, result.CompressRatio*100)

	cc.recordMetrics(result)
	return result
}

// sortByRelevance æŒ‰ç›¸å…³æ€§æ’åº(è´¨é‡åˆ† + ç›¸ä¼¼åº¦)
func (cc *ContextCompressor) sortByRelevance(memories []Memory) []Memory {
	sorted := make([]Memory, len(memories))
	copy(sorted, memories)

	sort.Slice(sorted, func(i, j int) bool {
		// ä¼˜å…ˆçº§: è´¨é‡åˆ† > ç›¸ä¼¼åº¦ > æ–°æ—§
		scoreI := sorted[i].QualityScore*0.5 + sorted[i].Similarity*0.5
		scoreJ := sorted[j].QualityScore*0.5 + sorted[j].Similarity*0.5

		if scoreI != scoreJ {
			return scoreI > scoreJ // é«˜åˆ†ä¼˜å…ˆ
		}

		// åŒåˆ†æ•°,æ–°çš„ä¼˜å…ˆ
		return sorted[i].UpdatedAt.After(sorted[j].UpdatedAt)
	})

	return sorted
}

// estimateTokens ä¼°ç®—å­—ç¬¦ä¸²çš„tokenæ•°(ç®€å•çº¿æ€§ä¼°è®¡)
func (cc *ContextCompressor) estimateTokens(s string) int {
	if s == "" {
		return 0
	}

	// ä¸­æ–‡: 1ä¸ªå­—ç¬¦ = 1.3ä¸ªtoken
	// è‹±æ–‡: 4ä¸ªå­—ç¬¦ = 1ä¸ªtoken
	chineseCount := 0
	englishCount := 0

	for _, ch := range s {
		if ch >= 0x4E00 && ch <= 0x9FFF {
			chineseCount++
		} else if (ch >= 'a' && ch <= 'z') || (ch >= 'A' && ch <= 'Z') {
			englishCount++
		}
	}

	tokens := int(float64(chineseCount)*1.3 + float64(englishCount)/4.0)
	if tokens == 0 && len(s) > 0 {
		tokens = 1
	}

	return tokens
}

// ===== Deduplicator Methods =====

// IsDuplicate æ£€æŸ¥æ˜¯å¦é‡å¤
func (d *Deduplicator) IsDuplicate(content string) bool {
	d.mu.RLock()
	defer d.mu.RUnlock()

	normalized := strings.TrimSpace(strings.ToLower(content))

	// ç²¾ç¡®åŒ¹é…
	if d.seenContent[normalized] {
		return true
	}

	// ç›¸ä¼¼åº¦æ£€æŸ¥(ç®€å•å®ç°)
	for seen := range d.seenContent {
		if d.calculateSimilarity(normalized, seen) > d.similarity {
			return true
		}
	}

	return false
}

// Add æ·»åŠ å†…å®¹åˆ°å»é‡é›†åˆ (LRUæ·˜æ±°)
func (d *Deduplicator) Add(content string) {
	d.mu.Lock()
	defer d.mu.Unlock()

	normalized := strings.TrimSpace(strings.ToLower(content))

	// å¦‚æœå·²å­˜åœ¨,ä¸é‡å¤æ·»åŠ 
	if d.seenContent[normalized] {
		return
	}

	// âœ… ä¿®å¤: LRUæ·˜æ±°æœºåˆ¶ (é˜²æ­¢æ— é™å¢é•¿)
	// å½“è¾¾åˆ°æœ€å¤§å®¹é‡æ—¶,åˆ é™¤æœ€æ—§çš„æ¡ç›®
	if len(d.seenContent) >= d.maxSize {
		if len(d.addedOrder) > 0 {
			oldest := d.addedOrder[0]
			delete(d.seenContent, oldest)
			d.addedOrder = d.addedOrder[1:]

			log.Printf("  ğŸ—‘ï¸ LRUæ·˜æ±°: åˆ é™¤æœ€æ—§çš„æ¡ç›® (%då­—ç¬¦)",
				len(oldest))
		}
	}

	// æ·»åŠ æ–°å†…å®¹
	d.seenContent[normalized] = true
	d.addedOrder = append(d.addedOrder, normalized)
}

// calculateSimilarity è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦(Jaccard)
func (d *Deduplicator) calculateSimilarity(s1, s2 string) float64 {
	// ç®€å•çš„åŸºäºå•è¯çš„Jaccardç›¸ä¼¼åº¦
	words1 := strings.Fields(s1)
	words2 := strings.Fields(s2)

	if len(words1) == 0 || len(words2) == 0 {
		return 0
	}

	// è½¬æ¢ä¸ºSet
	set1 := make(map[string]bool)
	set2 := make(map[string]bool)

	for _, w := range words1 {
		set1[w] = true
	}
	for _, w := range words2 {
		set2[w] = true
	}

	// è®¡ç®—äº¤é›†å’Œå¹¶é›†
	intersection := 0
	union := len(set1) + len(set2)

	for w := range set1 {
		if set2[w] {
			intersection++
			union--
		}
	}

	if union == 0 {
		return 0
	}

	return float64(intersection) / float64(union)
}

// Clear æ¸…ç©ºå»é‡é›†åˆ
func (d *Deduplicator) Clear() {
	d.mu.Lock()
	defer d.mu.Unlock()

	d.seenContent = make(map[string]bool)
}

// ===== Metrics Methods =====

func (cc *ContextCompressor) recordMetrics(result CompressionResult) {
	cc.metrics.mu.Lock()
	defer cc.metrics.mu.Unlock()

	n := cc.metrics.CompressionRuns + 1

	// æ›´æ–°å¹³å‡å€¼
	oldAvgInput := cc.metrics.AvgInputTokens * float64(cc.metrics.CompressionRuns)
	cc.metrics.AvgInputTokens = (oldAvgInput + float64(result.InputTokens)) / float64(n)

	oldAvgOutput := cc.metrics.AvgOutputTokens * float64(cc.metrics.CompressionRuns)
	cc.metrics.AvgOutputTokens = (oldAvgOutput + float64(result.OutputTokens)) / float64(n)

	oldAvgRatio := cc.metrics.AvgCompressionRatio * float64(cc.metrics.CompressionRuns)
	cc.metrics.AvgCompressionRatio = (oldAvgRatio + result.CompressRatio) / float64(n)

	cc.metrics.CompressionRuns = n
	cc.metrics.TotalRemoved += int64(result.RemovedCount)
	now := time.Now()
	cc.metrics.LastCompressionAt = &now
}

// GetMetrics è·å–å‹ç¼©æŒ‡æ ‡
func (cc *ContextCompressor) GetMetrics() CompressionMetrics {
	cc.metrics.mu.RLock()
	defer cc.metrics.mu.RUnlock()

	// è¿”å›ä¸åŒ…å«é”çš„å‰¯æœ¬
	return CompressionMetrics{
		CompressionRuns:     cc.metrics.CompressionRuns,
		AvgInputTokens:      cc.metrics.AvgInputTokens,
		AvgOutputTokens:     cc.metrics.AvgOutputTokens,
		AvgCompressionRatio: cc.metrics.AvgCompressionRatio,
		TotalRemoved:        cc.metrics.TotalRemoved,
		LastCompressionAt:   cc.metrics.LastCompressionAt,
	}
}

// idPrefix å®‰å…¨åœ°è·å–IDå‰ç¼€(æœ€å¤š8ä¸ªå­—ç¬¦)
func idPrefix(id string) string {
	if len(id) > 8 {
		return id[:8]
	}
	return id
}

// PrintStats æ‰“å°ç»Ÿè®¡ä¿¡æ¯
func (cc *ContextCompressor) PrintStats() {
	metrics := cc.GetMetrics()

	log.Println("\nğŸ“¦ ä¸Šä¸‹æ–‡å‹ç¼©ç»Ÿè®¡:")
	log.Println(strings.Repeat("â•", 60))
	log.Printf("  å‹ç¼©æ¬¡æ•°: %d\n", metrics.CompressionRuns)

	if metrics.CompressionRuns > 0 {
		log.Printf("  å¹³å‡è¾“å…¥: %.0f tokens\n", metrics.AvgInputTokens)
		log.Printf("  å¹³å‡è¾“å‡º: %.0f tokens\n", metrics.AvgOutputTokens)
		log.Printf("  å¹³å‡å‹ç¼©ç‡: %.1f%%\n", metrics.AvgCompressionRatio*100)
		log.Printf("  æ€»ç§»é™¤: %dæ¡\n", metrics.TotalRemoved)
	}

	if metrics.LastCompressionAt != nil {
		log.Printf("  æœ€åå‹ç¼©: %s\n", metrics.LastCompressionAt.Format("2006-01-02 15:04:05"))
	}

	log.Println(strings.Repeat("â•", 60))
}

// ContextCompressionExample ä½¿ç”¨ç¤ºä¾‹
func ContextCompressionExample() {
	// åˆ›å»ºå‹ç¼©å™¨(ç›®æ ‡700 tokens)
	compressor := NewContextCompressor(700)

	// æ¨¡æ‹ŸMem0è¿”å›çš„åŸå§‹ç»“æœ(3400+ tokens)
	memories := []Memory{
		{
			ID:             "m1",
			Content:        "è¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„ç›¸ä¼¼äº¤æ˜“è®°å½•,åŒ…å«è¯¦ç»†çš„æ‰§è¡Œè¿‡ç¨‹å’Œç»“æœåˆ†æ...",
			Type:           "decision",
			QualityScore:   0.95,
			Similarity:     0.92,
			Status:         "evaluated",
		},
		{
			ID:             "m2",
			Content:        "å¦ä¸€ä¸ªç›¸ä¼¼çš„äº¤æ˜“,ä½†è´¨é‡ç¨ä½...",
			Type:           "decision",
			QualityScore:   0.72,
			Similarity:     0.88,
			Status:         "evaluated",
		},
		// ... æ›´å¤šè®°å¿†
	}

	// å‹ç¼©
	result := compressor.Compress(memories)

	// ä½¿ç”¨å‹ç¼©åçš„ç»“æœ
	fmt.Printf("âœ… å‹ç¼©å®Œæˆ:\n")
	fmt.Printf("   è¾“å…¥: %d tokens â†’ è¾“å‡º: %d tokens (æ¯”ç‡: %.1f%%)\n",
		result.InputTokens, result.OutputTokens, result.CompressRatio*100)
	fmt.Printf("   ä¿ç•™: %dæ¡, ç§»é™¤: %dæ¡, å»é‡: %dæ¡\n",
		len(result.Memories), result.RemovedCount, result.DeduplicatedCount)
}
